{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.0.2-bin-hadoop2.7'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "# pyspark==2.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import sum,avg,max,min,mean\n",
    "from pyspark.sql.functions import row_number,lit\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADMISSIONS      = spark.read.option(\"header\",True).csv('C:\\Health data\\ADMISSIONS.csv')\n",
    "CHARTEVENTS     = spark.read.option(\"header\",True).csv('C:\\Health data\\CHARTEVENTS.csv')\n",
    "DIAGNOSES_ICD   = spark.read.option(\"header\",True).csv('C:\\Health data\\DIAGNOSES_ICD.csv')\n",
    "D_ICD_DIAGNOSES = spark.read.option(\"header\",True).csv('C:\\Health data\\D_ICD_DIAGNOSES.csv')\n",
    "ICUSTAYS        = spark.read.option(\"header\",True).csv('C:\\Health data\\ICUSTAYS.csv')\n",
    "LABEVENTS       = spark.read.option(\"header\",True).csv('C:\\Health data\\LABEVENTS.csv')\n",
    "OUTPUTEVENTS    = spark.read.option(\"header\",True).csv('C:\\Health data\\OUTPUTEVENTS.csv')\n",
    "PATIENTS        = spark.read.option(\"header\",True).csv('C:\\Health data\\PATIENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName('SparkApp').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_patients_num   = PATIENTS.select('SUBJECT_ID').distinct().count() # how many patients\n",
    "total_Deceased_patients = PATIENTS.filter(\"EXPIRE_FLAG == '1'\").drop('DOD_HOSP','DOD_SSN')\n",
    "total_alive_patients    = PATIENTS.filter(\"EXPIRE_FLAG == '0'\").drop('DOD_HOSP','DOD_SSN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine three database: LABEVENTS, OUTPUTEVENTS, and CHARTEVENTS ############  \n",
    "LABEVENTS = LABEVENTS.select('SUBJECT_ID','ITEMID','CHARTTIME','VALUENUM').na.drop()\n",
    "LABEVENTS = LABEVENTS.withColumnRenamed(\"VALUENUM\", \"VALUE\")\n",
    "OUTPUTEVENTS = OUTPUTEVENTS.select('SUBJECT_ID','ITEMID','CHARTTIME','VALUE').na.drop()\n",
    "CHARTEVENTS = CHARTEVENTS.select('SUBJECT_ID','ITEMID','CHARTTIME','VALUENUM').na.drop()\n",
    "CHARTEVENTS = CHARTEVENTS.withColumnRenamed(\"VALUENUM\", \"VALUE\")\n",
    "\n",
    "infor1 = OUTPUTEVENTS.union(LABEVENTS)\n",
    "infor2 = infor1.union(CHARTEVENTS)\n",
    "infor2 = infor2.withColumnRenamed(\"SUBJECT_ID\", \"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-map the feature ID ##############\n",
    "Festure_ID = infor2.select('ITEMID').distinct()\n",
    "w = Window().orderBy(lit('A'))\n",
    "Festure_ID = Festure_ID.withColumn(\"rowNum\", row_number().over(w)).withColumnRenamed(\"ITEMID\", \"ITEMID1\") \n",
    "infor2 = infor2.join(Festure_ID, infor2.ITEMID==Festure_ID.ITEMID1,\"left\")\\\n",
    "               .select(\"ID\",\"CHARTTIME\",\"VALUE\",\"rowNum\")\\\n",
    "               .withColumnRenamed(\"rowNum\", \"ITEMID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SVMLight format for alive patients ###################\n",
    "Alive_patients_infor = total_alive_patients.join(infor2, total_alive_patients.SUBJECT_ID == infor2.ID, \"left\")\n",
    "Alive_patients_infor = Alive_patients_infor.withColumn(\"VALUE\", Alive_patients_infor[\"VALUE\"].cast(\"double\"))\n",
    "Alived = Alive_patients_infor.select('ID','ITEMID','VALUE').groupBy(\"ID\",\"ITEMID\").mean(\"VALUE\")\n",
    "Alived = Alived.withColumn(\"target\", lit(0)).withColumnRenamed(\"avg(VALUE)\", \"VALUE\")\n",
    "Alived = Alived.filter(Alived.VALUE != 0)\n",
    "Alived = Alived.withColumn(\"ITEMID\", Alived[\"ITEMID\"].cast(\"int\"))\n",
    "\n",
    "hu = Alived.select('ID').distinct()\n",
    "w = Window().orderBy(lit('A'))\n",
    "hu = hu.withColumn(\"rowNum\", row_number().over(w)).withColumnRenamed(\"ID\", \"ID1\") \n",
    "Alived_data = Alived.join(hu,Alived.ID==hu.ID1,\"left\")\n",
    "\n",
    "Alived_data1 = Alived_data.filter(Alived_data.rowNum <= 10000)\n",
    "Alived_data1 = Alived_data1.select('ID','ITEMID','target','VALUE')\n",
    "\n",
    "Alived_data2 = Alived_data.filter( (Alived_data.rowNum <= 20000) & (Alived_data.rowNum > 10000) )\n",
    "Alived_data2 = Alived_data2.select('ID','ITEMID','target','VALUE')\n",
    "\n",
    "hu1 = Alived_data1.sort(Alived_data1.ID.asc(),Alived_data1.ITEMID.asc()).groupBy(\"ID\", \"target\")\\\n",
    "    .agg(F.collect_list(F.struct(\"ITEMID\", \"VALUE\")).alias(\"feature_value\")) \\\n",
    "    .withColumn(\"feature_value\", F.expr(\"transform(feature_value, x -> concat_ws(':',x.ITEMID, x.VALUE))\")) \\\n",
    "    .withColumn(\"feature_value\", F.concat_ws(\"  \", F.col(\"feature_value\"))) \\\n",
    "    .withColumn(\"result\", F.concat_ws(\" \", F.col(\"Target\"), F.col(\"feature_value\"))) \\\n",
    "    .select(\"result\")\n",
    "\n",
    "hu1.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"append\").save(\"output_1.txt\")\n",
    "\n",
    "hu2 = Alived_data2.sort(Alived_data2.ID.asc(),Alived_data2.ITEMID.asc()).groupBy(\"ID\", \"target\")\\\n",
    "    .agg(F.collect_list(F.struct(\"ITEMID\", \"VALUE\")).alias(\"feature_value\")) \\\n",
    "    .withColumn(\"feature_value\", F.expr(\"transform(feature_value, x -> concat_ws(':',x.ITEMID, x.VALUE))\")) \\\n",
    "    .withColumn(\"feature_value\", F.concat_ws(\"  \", F.col(\"feature_value\"))) \\\n",
    "    .withColumn(\"result\", F.concat_ws(\" \", F.col(\"Target\"), F.col(\"feature_value\"))) \\\n",
    "    .select(\"result\")\n",
    "\n",
    "hu2.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"append\").save(\"output_2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SVMLight format for deceased patients ###################\n",
    "Day_before_Deceased = 30\n",
    "index_Date_Deceased = total_Deceased_patients.select('SUBJECT_ID','DOD').withColumn('index_date', F.date_sub('DOD', Day_before_Deceased))\n",
    "index_Date = index_Date_Deceased.select('SUBJECT_ID','index_date')\n",
    "\n",
    "infor3 = infor2.join(index_Date, infor2.ID ==  index_Date.SUBJECT_ID, \"left\")\n",
    "infor4 = infor3.filter(infor3.CHARTTIME < infor3.index_date)\n",
    "\n",
    "infor4 = infor4.withColumn(\"VALUE\", infor4[\"VALUE\"].cast(\"double\")) \n",
    "Deceased = infor4.select('ID','ITEMID','VALUE').groupBy(\"ID\",\"ITEMID\").mean(\"VALUE\")\n",
    "Deceased = Deceased.withColumn(\"target\", lit(1)).withColumnRenamed(\"avg(VALUE)\", \"VALUE\")\n",
    "Deceased = Deceased.filter(Deceased.VALUE != 0)\n",
    "Deceased = Deceased.withColumn(\"ITEMID\", Deceased[\"ITEMID\"].cast(\"int\"))\n",
    "\n",
    "hu = Deceased.sort(Deceased.ID.asc(),Deceased.ITEMID.asc()).groupBy(\"ID\", \"target\")\\\n",
    "    .agg(F.collect_list(F.struct(\"ITEMID\", \"VALUE\")).alias(\"feature_value\")) \\\n",
    "    .withColumn(\"feature_value\", F.expr(\"transform(feature_value, x -> concat_ws(':',x.ITEMID, x.VALUE))\")) \\\n",
    "    .withColumn(\"feature_value\", F.concat_ws(\"  \", F.col(\"feature_value\"))) \\\n",
    "    .withColumn(\"result\", F.concat_ws(\" \", F.col(\"Target\"), F.col(\"feature_value\"))) \\\n",
    "    .select(\"result\")\n",
    "\n",
    "hu.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"append\").save(\"output.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
